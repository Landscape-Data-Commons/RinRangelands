---
title: 'Case Study: How Does Drought Impact Wind Erosion Risk?'
author: "Sarah McCord, Eric Jensen"
date: "2024-01-26"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview

Now we will use all of the skills we have learned in this workshop in a real world case study. This example walks through using R and publicly available datasets to examine how drought is influencing wind erosion in the Chihuahuan Desert. Our goal is to reproduce Figure 4 from McCord et al. 2023 (***https://doi.org/10.1002/ael2.20120***). First, we will query the Landscape Data Commons for data that fall within the Chihuahuan Desert ecoregion. Then, we will use these data to set a benchmark of expected wind erosion risk in the Chihuahuan Desert. With this benchmark in place, we will apply it to the monitoring data in the Landscape Data Commons and compare those results at multiple spatial scales. Finally, we will incorporate drought and remote-sensing based fractional cover data, to provide additional context to the field based monitoring data.

![](images/benchmark_combined_figure.png) 

Set your working directory 
make sure it ends in Part 2. If you tab over with a "" Rstudio will help you set it. 
```{r}
getwd() # check your current working drive
# setwd("") # change this if needed
```

Library the packages we will use in this demo
```{r}
library(tidyverse)
library(trex) # for fulling data on the Landscape Data Commons
library(sf) # for processing spatial data
library(httr2) # for Climate Engine API requests
library(mapview) # for visualizing spatial data
library(ggpubr) # for combining multiple ggplot objects
library(maps)
```

## 1. Prepare Data

Now we need to pull in some polygons for our region of interest so that we can set a spatial boundaries on our data.  
```{r}
# Bring in the polygons for our region of interest polygons
chihuahuan_desert <- read_sf("MapLayers/ChihuahuanDesert.shp")
sandy_esg <- read_sf("MapLayers/SandyESG.shp")

# extract data from LDC based on the chihuahuan_desert polygon
data <- fetch_ldc_spatial(chihuahuan_desert, data_type = "indicators")
head(data)
```

## Lets check in with the study area and make a map of our regions of interest.
First lets make a map of the region of the boundaries of the chihuahuan desert within the US. To do this, we need to pull in a US state boundary. 
```{r}
# pull in US state boundaries
usa <- st_as_sf(maps::map("state", fill=TRUE, plot =FALSE))

# Generate a map from just the state outlines
state_base_map <- ggplot(usa) +
  geom_sf(color = "#2b2b2b", fill = "white", size=0.125)
state_base_map
```

Good start, lets add the Chihuahan desert study area on this map. 
Remember ggplot figures work like layers, so we can add the desert on top of our base map
```{r}
# lets add in the boundary for the Chihuahan desert 
# add this on to our prior map
state_map <- state_base_map +
  geom_sf(data = chihuahuan_desert, aes(fill = "Chihuahuan Desert"))+ 
  theme_minimal()
state_map
```
Nice. Now let's use some customization within ggplot to adjust legend position and change colors
```{r}
us_state_map <- state_base_map +
  geom_sf(data = chihuahuan_desert, aes(fill = "Chihuahuan Desert")) + # alpha makes the shape transparent
  scale_fill_manual(values = "goldenrod1", guide = guide_legend(title = "Legend")) + 
  theme(legend.position = "top")
us_state_map
```
Awesome, now we have a starting map. Lets move on to working with the data. 

Take a look at the LCD data and clean it up 
```{r}
# Make the DateVisited formatted as date
data <- data |> dplyr::mutate(DateVisited = lubridate::as_date(DateVisited),
                                    Year = lubridate::year(DateVisited) |> as.factor())
# Make sure the Lat/Long coordinates are numeric
data <- data |> 
  dplyr::mutate(Longitude_NAD83 = as.numeric(Longitude_NAD83),
                Latitude_NAD83 = as.numeric(Latitude_NAD83)) 
```



## 2. Establish benchmark
We want to understand which sites might be susceptible to wind erosion. 
To do this, we took a subset (~600) of the points in the ecoregion and compared them with wind erosion models and determined that decreases in total foliar cover below 30% are associated with an increase in wind erosion risk. However, we do not want to apply the benchmark to the points used to develop the benchmark. So we will first remove the benchmark generation points. Then we will apply the 30% benchmark to these points. 

![](images/Establishing_benchmark.jpg) 
pull in the benchmark development data
``` {r}
benchmark <- read.csv("benchmark_points.csv")
head(benchmark)
```

## 3. Apply benchmark
Start by removing the training benchmark data
``` {r}
# remove data that have been used in benchmarking
nrow(data)
data <- data |> subset(!PrimaryKey %in% benchmark$PrimaryKey)

# check total number of plots
nrow(data)
```
Score score the plots based on if they are meeting or not meeting benchmarks
``` {r}
# Apply the benchmark of 30% to the Total Foliar Cover values
data <- data |> 
  dplyr::mutate(evaluated = dplyr::case_when(
    TotalFoliarCover >30 ~ "Meeting",
    TotalFoliarCover <=30 ~ "Not meeting")) |>
  dplyr::filter(!is.na(TotalFoliarCover))
# this will create a new column called evaluated with values of meeting or not meeting based on the values in the Total Foliar Cover col
# there were some plots with NA in the TotalFoliarCover - we want to remove those since we cannot evaluate them for this benchmark
```

Convert these data to spatial objects so we can make maps and conduct spatial analyses
To accomplish this, we will need specify a CRS (Coordinate reference system)
[https://rspatial.org/raster/spatial/6-crs.html]
```{r}
# move from a st to sf object
# users must specify the columns which are coordinates, and the Coordinate reference system
data_sf <- sf::st_as_sf(data %>% subset(!is.na(Longitude_NAD83)),
                     coords = c("Longitude_NAD83", "Latitude_NAD83"),
                     crs = st_crs(4269)) # 4268 is the CRS code for NAD 83


# save these data as a shapefile
sf::st_write(data_sf, "ldc_data.shp", append=F) 
```
We can add points to our base map with states
```{r}
# recall study area map
us_state_map
```
This map is great for providing context of our study are in the US.... but to see the plot points on a map we are going to need to zoom into New Mexico and AZ. To do that, we can filter the 'usa' data frame
```{r}
head(usa) # seems like the state name is in the ID column 

zoomed_state_map <- ggplot()+
  geom_sf(data=usa %>%  dplyr::filter(ID %in% c("new mexico", "arizona")),
          fill = "white", color = "black")+
  theme_minimal()
zoomed_state_map
```
Add in the Chihuahan Desert and Sandy ESG map layers to this zoomed in map
```{r}
zoomed_state_map <- zoomed_state_map+ 
  geom_sf(data = chihuahuan_desert, aes(fill = "Chihuahuan Desert"), alpha = 0.75) + # make semi transparent with alpha 
  geom_sf(data = sandy_esg, aes(fill = "Sandy ESG"), color = "gold4" )+ 
  scale_fill_manual(values = c("Chihuahuan Desert" = "goldenrod1", 
                               "Sandy ESG" = "gold4"),
                    name = "",
                    labels = c("Chihuahuan Desert", "Sandy ESG")) + 
  theme(legend.position = "top")
zoomed_state_map
```
##### challenge!  
# add the plot locations to the study area map
consider changing dot size using "size =" 
```{r}
#zoomed_state_map_withPts <- zoomed_state_map + 
```



Now we can see the plot locations, but I wonder where plots are that are meeting or not meeting the benchmark? 
We can change the color of each plot location based on the 'evaluated' column
```{r}
# get the values in this column 
summary(as.factor(data_sf$evaluated))

# Specify colors for meeting and not meeting
meeting_colors <- c("Meeting" = "royalblue1", "Not meeting" = "firebrick4")

# add these points to the zoomed state map, but have the color of each point indicate if the location is meeting or not meeting the benchmark
study_with_bench_eval <- zoomed_state_map +
  geom_sf(data = data_sf,
          size = 0.75, 
          aes(color = evaluated)) +
  scale_color_manual(values = meeting_colors, 
                     name = "Wind Erosion Benchmark")

# Display the map with adjusted point size and color
study_with_bench_eval
```

Now that we know where these points are, we can make some summary graphs to show the portion of the plots which are meeting this benchmark.

For this, we are interested in understanding how many plots within the Chihuahuan Desert, the Sandy ESG, MLRA 42, and a long-term research site at the Jornada Experimental Range called "NWERN_JER"). 
``` {r}
# Identify the membership of each plot within different polygons (Chihuahuan Desert, MLRA 42, SandyESG and a long-term research site at the Jornada Experimental Range called "NWERN_JER")
# We know all the points are within the Chihuahaun desert, do create a col and fill all values with yes
# sort plots by if they occur in MLRA42. If the value in mlrasym col is 42, put 'yes' in the new column called MLRA42
data_sf <- data_sf |> 
  dplyr::mutate(Chihuahuan_desert = "yes",
                MLRA42 = dplyr::case_when(mlrarsym %in% "42"~ "yes"),
                NWERN_JER = dplyr::case_when(ProjectKey %in% "NWERN_JER"~ "yes"))
```
we also want to know if the points falls within the Sandy ESG. We will use the st_within function 
```{r}
# create a col called SandyESG, where values correspond to if each row is within the SandyESG
# Assuming data_sf and sandy_sf are sf objects
data_sf$SandyESG <- sf::st_within(data_sf, sandy_esg)
```
hmm seems like there is an issue about mismatching CRS
```{r}
# figure out the current crs for each data 
print(st_crs(data_sf))
print(st_crs(sandy_esg))
```

Ah yes the crs for sandy_esg is WGS 84. Let's change that. 
```{r}
# Transform sandy_esg_transformed to have the same CRS as nm_data
sandy_esg_transformed <-  sf::st_make_valid(sf::st_transform(sandy_esg, st_crs(data_sf)))

# try again to assign values based on points, but use the transformed data
data_sf$SandyESG <- sf::st_within(data_sf, sandy_esg_transformed)

head(summary(data_sf$SandyESG), 20) # the values in this col are either 0 or 1

# Convert the logical values to "Yes" or "No" if needed
data_sf$SandyESG <- ifelse(data_sf$SandyESG, "yes", "No")
head(summary(data_sf$SandyESG), 20)

```
Great. Now for each plot (row), we know if the plot occurs within the Chihuahan desert, Sandy ESG, MLRA 42, and in NWERN

To prepare for plotting, we need to transform data from wide to long format using a pivot. 
```{r}
# To faciliate plotting, it would be helpful if the data were long rather than wide.
data_long <- as.data.frame(data_sf) |>
  tidyr::pivot_longer(cols = c("MLRA42",
                               "SandyESG", 
                               "NWERN_JER", 
                               "Chihuahuan_desert"),
                      names_to = "region",
                      values_to = "region_membership") |>
  subset(region_membership =="yes")|>
  dplyr::mutate(region = factor(region)|>
                  dplyr::recode( "NWERN_JER" = "Site",
                                 "SandyESG" = "ESG", 
                                 "MLRA42" = "MLRA",
                                 "Chihuahuan_desert" = "Ecoregion"))

# We also need region to be a factor for easier plot formatting
data_long$region <- factor(data_long$region,
                              levels = c( "Site", 
                                          "ESG",
                                          "MLRA",
                                          "Ecoregion")
                              )
```

### Plot the results
# and complete a challenge. 
# We subset the data to 2015-2022 because that is when we have a sufficient sample size
``` {r}
# box plot
benchmark_boxplot <- 
  ggplot(data_long |> 
           subset(Year %in% c("2015", "2016", "2017", "2018", "2019", "2020", "2021", "2022") &
                                TotalFoliarCover),
       aes(x = Year,
           y = TotalFoliarCover,
           group = Year)) +
  facet_grid(rows = "region")+
  geom_boxplot()+
  geom_point(aes(color = evaluated))+
  scale_color_manual(values = c("darkblue", "orange"))+
  theme_bw()+
  ylab("Total foliar cover (%)")+
  guides(color=guide_legend(title="Benchmark status"))
benchmark_boxplot
```

###### CHALLENGE 
# this looks good.. but the points are covering up the boxes. To make this easier to see, make the points semi transparents and wiggle (jitter) the points around
```{r}
#benchmark_boxplot_jittered <- 
```  
# bar chart
```{r}
benchmark_barchart <- 
  ggplot(data_long |> 
           subset(Year %in% c("2015", "2016", "2017", "2018", "2019", "2020", "2021", "2022") &
                                TotalFoliarCover),
       aes(x = Year,
           y = TotalFoliarCover,
           fill = evaluated)) +
  facet_grid(rows = "region")+
  geom_bar(position = "fill", stat = "identity")+
  scale_fill_manual(values = c("darkblue", "orange"))+
  theme_bw()+
  ylab("Proportion of plots")+
  guides(fill=guide_legend(title="Benchmark status"))
benchmark_barchart
```

# summarize these data in a table 
```{r}
data_long_summary <- 
  data_long |> 
  subset(Year %in% c("2015", "2016", "2017", "2018", "2019", "2020", "2021", "2022") &
           TotalFoliarCover) |>
  dplyr::select(PrimaryKey, region, Year) |>
  dplyr::mutate(Year = as.character(Year)) |>
  dplyr::distinct()|>
  dplyr::group_by(Year, region) |> 
  dplyr::tally()
```




## 4. Climate Data/RAP
### Authenticate to API and run preliminary test

Define Climate Engine API key and run a simple test endpoint to make sure that the key authenticated correctly. The Climate Engine API documentation is published at https://docs.climateengine.org

NOTE: The key below will remain active through February 4, please request a free key to run this in the future: https://docs.climateengine.org/docs/build/html/registration.html

```{r}
# Define root url for Climate Engine API
root_url <- "https://api.climateengine.org/"

# Define key
key <- "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJmcmVzaCI6ZmFsc2UsImlhdCI6MTcwNTg3ODY2NiwianRpIjoiYmEyYTZmMzMtNjdkOS00ODAzLThiOTQtYWQ0Yjg5NmZjZDJkIiwibmJmIjoxNzA1ODc4NjY2LCJ0eXBlIjoiYWNjZXNzIiwic3ViIjoidUZXM0VmMUxhWmI5U0VhUnRPdzVNUzI2UjBTMiIsImV4cCI6MTcwNzE3NDY2Niwicm9sZXMiOiJ1c2VyIiwidXNlcl9pZCI6InVGVzNFZjFMYVpiOVNFYVJ0T3c1TVMyNlIwUzIifQ.WIQZIn97BpFZorL_n6NGvEi_MHg2qMnIUh9Rm6aOcl0"

# Define endpoint
endpoint <- '/home/validate_key'

# Run simple endpoint to get key expiration
test <- request(base_url = paste0(root_url, endpoint)) |>
  req_headers(Authorization = key) |>
  req_perform()

print(resp_raw(test))

# Clean up unneeded objects
rm(test, endpoint)
```


### Import the wind erosion area of interest

This section of the notebook runs using two sample areas of interest for the Sandy Ecological Site Group and for the Chihuahuan Desert ecoregion, but you can run similar analysis for any shapefile. We will import the shapefile as an sf object and upload it to Google Earth Engine and then visualize thee seleted area of interest (aoi) on a leaflet map using the mapview package. You can also modify which area of interest you want to analyze in this code chunk.

```{r}
# Set AOI name as either 'Sandy_ESG' or 'Chihuahuan_Desert'
aoi_name = 'Sandy_ESG'

# Import the Sandy ESG shapefile and select an individual area of interest. 
aoi <- st_read(paste0('MapLayers/', aoi_name |> str_remove('_'), '.shp')) |>
  st_simplify(preserveTopology = TRUE) |> 
  st_transform(crs = "OGC:CRS84") 

# Define variables based on aoi_name
if(aoi_name == 'Sandy_ESG'){
  asset_id = 'users/EricRJensen-DRI/RinRangelands/SandyESG'
  sub_choices = '["SandyESG"]'
  filter_by = 'id'
} else if(aoi_name == 'Chihuahuan_Desert'){
  asset_id = 'users/EricRJensen-DRI/RinRangelands/ChihuahuanDesert'
  sub_choices = '["Chihuahuan Semi-Desert Province"]'
  filter_by = 'PROVINCE'
}

# Visualize the allotments on a leaflet map using mapview package
mapview(aoi)
```


### Make API requests to get Rangeland Analysis Platform (RAP) fractional vegetation cover data for annual forbs and grasses (AFG), perennial forbs and grasses (PFG), shrubs (SHR), trees (TRE), and bare ground (BGR)

#### Fetch RAP data

Here we will iterate over a list of vegetation types to get an annual timeseries of vegetation cover for our area of interest. There are a few steps included here to clean up the data returned from the API. Requests are sent to the API in the JSON format and the resulting data is returned in the JSON format, so several lines of code convert the JSON data into an R dataframe.

Resources:
- API Documentation for the endpoint used in this section is found here: https://docs.climateengine.org/docs/build/html/timeseries.html#rst-timeseries-native-custom-asset
- API Documentation for the Rangeland Analysis Platform dataset is found here: https://docs.climateengine.org/docs/build/html/variables.html#rst-rap-vegetation-cover
- Documentation on the Rangeland Analysis Platform is available through https://rangelands.app

```{r}
# Define Climate Engine API Point 
endpoint <- 'timeseries/native/custom_asset'

# Define dataset for function below
dataset <- "RAP_COVER"

# Make a list of RAP variable options — we will loop over these using the function below to return all of the data
variables <- c('AFG', 'BGR', 'PFG', 'SHR', 'TRE')
 
# Function to return data frame of RAP time-series
get_rap_df <- function(variable){
  
  print(paste0("Requesting data for ", variable))
  
  # Create list of API parameters for request
  params <- list(
    dataset = dataset,
    variable = variable,
    start_date = '1986-01-01',
    end_date = '2022-12-31',
    area_reducer = 'mean',
    asset_id = asset_id,
    sub_choices = sub_choices,
    filter_by = filter_by
    )
  
  # Make API request and get data from response
  data <- request(base_url = paste0(root_url, endpoint)) |> # define URL to make request to
    req_url_query(query = !!!params) |> # define paramaters for request
    req_headers(Authorization = key) |> # set authentication key as a header for request
    req_perform() |> # perform request
    resp_body_json() |> # parse result into JSON
    pluck(1, 'Data') # Get RAP data from the JSON
  
  # Convert JSON to dataframe
  # For each date in data response, convert to one row dataframe and then bind them together
  generate_data_frame <- function(item){
    
    return(tibble(Date = as.Date(item$Date),
                  Variable = as.factor(names(item[2]) |> str_sub(1,4)),
                  Value = item[[2]])) }
  df <- purrr::map(data, generate_data_frame) |> 
    bind_rows()
  
  return(df) }

# Map API function over list of RAP variables to get timeseries
rap_df <- purrr::map(variables, get_rap_df) |>
  bind_rows() # Combine list of dataframes into an individual dataframe

print(rap_df)
```


#### Visualize the RAP data as timeseries similar for each vegetation type

Make simple plots of the RAP time-series data for the Chihuahuan Desert or Sandy ESG, depending on the user selection. The plot produces is similar to the one produced in the RAP web application.

Note: We could also visualize the data as a stacked bar chart, similar to with the drought data below, but decided to visualize them as lines do do something different.

```{r}
# Make a simple plot of the time-series data
rap_cover <- ggplot(data = rap_df)+
  geom_line(mapping = aes(x = Date, y = Value, colour = Variable), size =2)+
  labs(title = paste0('Rangeland Analysis Platform (RAP) Vegetation Cover for ', 
                      aoi_name |> str_replace('_', ' ')), y = 'Fractional cover (%)') +
  scale_colour_manual(values = c("#BF604A", "#BF8C4A", "#A5C737", "#3757AD", "#0A6E1C"), 
                      labels = c("AFG", "BGR", "PFG", "SHR", "TRE"))+
  theme_classic()+
  scale_y_continuous(expand = c(0,0))+
  theme(legend.position="bottom",
        axis.title.x = element_blank(),
        legend.title=element_blank())
rap_cover


# add a line for 2015, since the rest of our benchmark comparison focused on 2015-2022 
rap_cover<- rap_cover+ 
  geom_vline(xintercept = as.Date("2015-01-01"), linetype = "dashed", 
             color = "black", size = 1)
rap_cover

```
## Challenge 
For each year in the ROI, Calculate total  RAP cover of vegetation (AFG, PFG, SHR, TRE). Then apply our wind erosion benchmark year: determine the number of years that the total  vegetation cover is 70% or higher (meeting the benchmark), or lower (not meeting) 

* Hint, you might need to use filters, groups and summarize/mutate functions to calculate vegetation cover. 
```{r}


```
Answer: Between 1986 - 2022, annual average cover in the SandyESG was at least 30% total vegetation cover and was therefore meeting out wind erosion benchmark in 20 years. 17 years were not meeting the benchmark. 









### Make API request to get Standardized Precipitation-Evapotranspiration Index (SPEI) data from gridMET Drought.

#### Fetch 180-day SPEI data from gridMET Drought.

Here, we will get histograms of 180-day SPEI drought data based on accepted drought categories developed by NOAA NIDIS (drought.gov). This will allow us to make a beautiful stacked bar chart to depict the drought history for our AOI. You can learn more about drought indicators and how the Climate Engine team is helping the BLM to assess drought on our support site: https://support.climateengine.org/article/128-rangeland-drought-assessment

NOTE: I've pre-run the fetching of the histogram data since it can be time-consuming to run live. Skip ahead to the next section to read-in CSVs of the drought data to produce the stacked bar chart.

```{r}
# # Generate date list to to extract statistics over
# start_year = 2015
# end_year = 2023
# yearlist = seq(from = start_year, to = end_year, by = 1)
# generate_dates <- function(year){
#   dates_year <- seq(from = as.Date(paste0(as.character(year), "/1/5")), to = as.Date(paste0(as.character(year), "/12/31")), by = '5 days')
#   return(c(as.character(dates_year))) }
# dates <- map(yearlist, generate_dates) |> 
#   flatten_chr()
# 
# # Define Climate Engine API Point 
# # Documentation for this endpoint found here: https://docs.climateengine.org/docs/build/html/zonal_statistics.html#rst-zonal-stats-pixel-count-custom-asset
# endpoint <- 'zonal_stats/pixel_count/custom_asset'
# 
# # Documentation on the gridMET Drought is available here: https://support.climateengine.org/article/45-gridmet-drought
# # API Documentation on the Rangeland Analysis Platform: https://docs.climateengine.org/docs/build/html/variables.html#rst-gridmet-drought
# dataset <- 'GRIDMET_DROUGHT'
# variable <- 'spei180d'
# 
# # Generate dataframe of histograms
# get_drought_histogram <- function(date){
# 
#   print(paste0('Running ', date))
# 
#   # Create list of API parameters for request
#   params = list(
#     dataset = dataset,
#     variable = variable,
#     end_date = as.character(date),
#     area_reducer = 'mean',
#     asset_id = asset_id,
#     sub_choices = sub_choices,
#     filter_by = filter_by,
#     bins = '[-2, -1.5, -1.2,-0.7, -0.5, 0.5, 0.7, 1.2, 1.5, 2]')
#   
#   # Make API request and get data from response
#   data <- request(base_url = paste0(root_url, endpoint)) |>
#     req_url_query(query = !!!params) |>
#     req_headers(Authorization = key) |>
#     req_perform() |>
#     resp_body_json() |>
#     pluck(1)
#   
#   # Iterate over the SPEI histogram bins to produce dataframe of SPEI histograms
#   bins = names(data$spei180d)
#   row_df <- tibble('Date' = date)
#   for (bin in bins){
#     val = data$spei180d[bin][1] |> 
#       unlist() |> 
#       unname()
#     df = tibble(bin = val)
#     names(df) <- bin
#     row_df = bind_cols(row_df, df)}
#   
#   return(row_df) }
# 
# spei_df <- map(dates, get_drought_histogram) |> 
#   bind_rows() %>%
#   mutate(across(everything(), replace_na, 0))
# 
# # Optionally write out csv
# # write_csv(spei_df, paste0('data/', aoi_name, '_', variable, '_', as.character(start_year), '_', as.character(end_year), '.csv'))
```


#### Create a stacked barchart of the 180-day SPEI drought categories for the AOI

In the code below we will prepare the dataframe of drought histograms that we produced in the last section for visualization. First, we will pivot the dataframe from wide to long; then, we will calculate the percentage of the area of interest that is in each drought category for each date; last, we will order the categories in the correct order to match the figure referenced at the beginning of this case study.

```{r}
# Optionally read in 180-day SPEI CSV
# If you skipped the above section, you'll need the variables below:
start_year = 2015
end_year = 2023
variable <- 'spei180d'
spei_df <- read_csv(paste0('data/', aoi_name, '_', variable, '_', as.character(start_year), '_', as.character(end_year), '.csv'))

# Pivot spei_df longer for visualizing, select drought categories, and calculate percent of area in each category
spei_df_plot <- spei_df |> 
  select(-'null') |> # remove null column resulting from gridMET drought data issue
  pivot_longer(!Date, names_to = 'Category', values_to = 'Count') |> # pivot from wide to long dataframe
  group_by(Date) |>
  mutate(Pixels = sum(Count)) |> # calculate sum of pixels for each date for generating histogram
  ungroup() |>
  mutate(Category = factor(Category, levels = c('0', '-1', '-2', '-3', '-4', '-5', '1', '2', '3', '4', '5'))) |> # convert Drought Category to a factor for plotting
  mutate(Percent = (Count/Pixels) * 100) |> # calculate percent of pixels in each class for generating histogram
  filter(Category %in% c('0', '-1', '-2', '-3', '-4', '-5')) # filter for drought categories


# make a figure to display SPEI in the sandy ESG
spei_fig <- ggplot(spei_df_plot, mapping = aes(x = Date, y = Percent, fill = Category))+
            geom_bar(stat = 'identity', width = 10)+
             scale_fill_manual(values = c('0' = '#FFFFFF', '-1' = '#FEFE33', '-2' = '#FFD580', '-3' = '#FFA500', '-4' = '#DC143C', '-5' = '#8C000F'), # define drought category colors according to NDMC: https://droughtmonitor.unl.edu/About/AbouttheData/DroughtClassification.aspx
                              labels = c('No drought', 'Abnormally\ndry', 'Moderate', 'Severe', 'Extreme', 'Exceptional'))+ # define drought category names according to NDMC: https://droughtmonitor.unl.edu/About/AbouttheData/DroughtClassification.aspx
            scale_y_continuous(breaks=c(0, 20, 40, 60, 80, 100), expand = c(0, 0)) +
  scale_x_date(expand = c(0, 0)) +  # the expand gets rid of gaps between x and y axis lines and the data 
            labs(title = paste0('180-day SPEI Timeseries Plot for ', aoi_name |> str_replace('_', ' ')), x = '', y = 'Percent', fill = 'Drought\nCategory')+
            theme_classic()+
            theme(legend.key=element_rect(colour="grey"))

spei_fig
```

```{r}
# Generate table of average and maximum area in drought by year and by category for AOI
options(scipen = 999)

spei_df_table <- spei_df |>
  mutate(Pixels = rowSums(across(where(is.numeric)))) |> # calculate total pixels in each row
  select('Date', 'D0' = '-1', , 'D1' = '-2', 'D2' = '-3', , 'D3' = '-4', 'D4' = '-5', 'Pixels') |> # remove non-drought columns
  mutate(D0 = ((D0 + D1 + D2 + D3 + D4) / Pixels) * 100,
         D1 = ((D1 + D2 + D3 + D4) / Pixels) * 100,
         D2 = ((D2 + D3 + D4) / Pixels) * 100,
         D3 = ((D3 + D4) / Pixels) * 100,
         D4 = ((D4) / Pixels) * 100) |> # calculate new columns for the total area in class or in more severe class as percentage
  mutate(Year = as.numeric(format(Date, '%Y'))) |> # create new column for year to group_by
  group_by(Year) # calculate maximum and average areas in drought by year for each category
  
# Get average area in each drought category (or greater) during each year and round to two decimals
spei_df_mean_table <- summarize_at(spei_df_table, c('D0', 'D1', 'D2', 'D3', 'D4'), ~round(mean(.), 2))

# Get maximum area in each drought category (or greater) during each year and round to two decimals
spei_df_max_table <- summarize_at(spei_df_table, c('D0', 'D1', 'D2', 'D3', 'D4'), max)
```



########### wrap it all together 
## make a combined figure! 
In R, we can also combine multiple ggplot objects to make a combined figure. 
recall the main figures we have created thus far
```{r}
us_state_map
zoomed_state_map
study_with_bench_eval
benchmark_boxplot
benchmark_barchart
rap_cover
spei_sandy
```

Combine four of these figures into one combined figure using ggarrange from ggpubr
```{r}
combined_benchmark_fig <- 
  ggarrange(study_with_bench_eval, benchmark_barchart, 
            rap_cover, spei_sandy, # specify the figures to include
            nrow = 2, ncol = 2, # arrangement of combined figures
            labels = "AUTO") # adds captiol letters for each graph panel
combined_benchmark_fig # looks smushed but that is because it is so big!! save it and look our output


ggsave("benchmark_combined_figure.png", height = 9, width = 13)
```

![](images/benchmark_combined_figure.png) 
