---
title: "R in Rangelands Climate Engine"
output: html_notebook
---

```{r}
library(tidyverse) # Data processing and visualization
library(sf) # Importing shapefile
library(httr2) # HTTP API requests
library(mapview) # Visualizing shapefile
```


## Authenticate to API and run preliminary test

Define Climate Engine API key and run a simple test endpoint to make sure that the key authenticated correctly. The Climate Engine API documentation is published at https://docs.climateengine.org

NOTE: The key below will remain active through February 4, please request a free key to run this in the future: https://docs.climateengine.org/docs/build/html/registration.html

```{r}
# Define root url for Climate Engine API
root_url <- "https://api.climateengine.org/"

# Define key
key <- "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJmcmVzaCI6ZmFsc2UsImlhdCI6MTcwNTg3ODY2NiwianRpIjoiYmEyYTZmMzMtNjdkOS00ODAzLThiOTQtYWQ0Yjg5NmZjZDJkIiwibmJmIjoxNzA1ODc4NjY2LCJ0eXBlIjoiYWNjZXNzIiwic3ViIjoidUZXM0VmMUxhWmI5U0VhUnRPdzVNUzI2UjBTMiIsImV4cCI6MTcwNzE3NDY2Niwicm9sZXMiOiJ1c2VyIiwidXNlcl9pZCI6InVGVzNFZjFMYVpiOVNFYVJ0T3c1TVMyNlIwUzIifQ.WIQZIn97BpFZorL_n6NGvEi_MHg2qMnIUh9Rm6aOcl0"

# Define endpoint
endpoint <- '/home/validate_key'

# Run simple endpoint to get key expiration
test <- request(base_url = paste0(root_url, endpoint)) |>
  req_headers(Authorization = key) |>
  req_perform()

print(resp_raw(test))

# Clean up unneeded objects
rm(test, endpoint)
```


## Import the wind erosion area of interest

This section of the notebook runs using two sample areas of interest for the Sandy Ecological Site Group and for the Chihuahuan Desert ecoregion, but you can run similar analysis for any shapefile. We will import the shapefile as an sf object and upload it to Google Earth Engine and then visualize thee seleted area of interest (aoi) on a leaflet map using the mapview package. You can also modify which area of interest you want to analyze in this code chunk.

```{r}
# Set AOI name as either 'Sandy_ESG' or 'Chihuahuan_Desert'
aoi_name = 'Sandy_ESG'

# Import the Sandy ESG shapefile and select an individual area of interest. 
aoi <- st_read(paste0('MapLayers/', aoi_name |> str_remove('_'), '.shp')) |>
  st_simplify(preserveTopology = TRUE) |> 
  st_transform(crs = "OGC:CRS84") 

# Define variables based on aoi_name
if(aoi_name == 'Sandy_ESG'){
  asset_id = 'users/EricRJensen-DRI/RinRangelands/SandyESG'
  sub_choices = '["SandyESG"]'
  filter_by = 'id'
} else if(aoi_name == 'Chihuahuan_Desert'){
  asset_id = 'users/EricRJensen-DRI/RinRangelands/ChihuahuanDesert'
  sub_choices = '["Chihuahuan Semi-Desert Province"]'
  filter_by = 'PROVINCE'
}

# Visualize the allotments on a leaflet map using mapview package
mapview(aoi)
```


## Make API requests to get Rangeland Analysis Platform (RAP) fractional vegetation cover data for annual forbs and grasses (AFG), perennial forbs and grasses (PFG), shrubs (SHR), trees (TRE), and bare ground (BGR)

Here we will iterate over a list of vegetation types to get an annual timeseries of vegetation cover for our area of interest. There are a few steps included here to clean up the data returned from the API. Requests are sent to the API in the JSON format and the resulting data is returned in the JSON format, so several lines of code convert the JSON data into an R dataframe.

Resources:
- API Documentation for the endpoint used in this section is found here: https://docs.climateengine.org/docs/build/html/timeseries.html#rst-timeseries-native-custom-asset
- API Documentation for the Rangeland Analysis Platform dataset is found here: https://docs.climateengine.org/docs/build/html/variables.html#rst-rap-vegetation-cover
- Documentation on the Rangeland Analysis Platform is available through https://rangelands.app

```{r}
# Define Climate Engine API Point 
endpoint <- 'timeseries/native/custom_asset'

# Define dataset for function below

dataset <- "RAP_COVER"

# Make a list of RAP variable options â€” we will loop over these using the function below to return all of the data
variables <- c('AFG', 'BGR', 'PFG', 'SHR', 'TRE')
 
# Function to return data frame of RAP time-series
get_rap_df <- function(variable){
  
  print(paste0("Requesting data for ", variable))
  
  # Create list of API parameters for request
  params <- list(
    dataset = dataset,
    variable = variable,
    start_date = '1986-01-01',
    end_date = '2022-12-31',
    area_reducer = 'mean',
    asset_id = asset_id,
    sub_choices = sub_choices,
    filter_by = filter_by
    )
  
  # Make API request and get data from response
  data <- request(base_url = paste0(root_url, endpoint)) |> # define URL to make request to
    req_url_query(query = !!!params) |> # define paramaters for request
    req_headers(Authorization = key) |> # set authentication key as a header for request
    req_perform() |> # perform request
    resp_body_json() |> # parse result into JSON
    pluck(1, 'Data') # Get RAP data from the JSON
  
  # Convert JSON to dataframe
  # For each date in data response, convert to one row dataframe and then bind them together
  generate_data_frame <- function(item){
    
    return(tibble(Date = as.Date(item$Date),
                  Variable = as.factor(names(item[2]) |> str_sub(1,4)),
                  Value = item[[2]])) }

  df <- map(data, generate_data_frame) |> 
    bind_rows()
  
  return(df) }

# Map API function over list of RAP variables to get timeseries
rap_df <- map(variables, get_rap_df) |>
  bind_rows() # Combine list of dataframes into an individual dataframe
print(rap_df)
```


## Visualize the RAP data as timeseries similar for each vegetation type

Make simple plots of the RAP time-series data for the Chihuahuan Desert or Sandy ESG, depending on the user selection. The plot produces is similar to the one produced in the RAP web application.

Note: We could also visualize the data as a stacked bar chart, similar to with the drought data below, but decided to visualize them as lines do do something different.

```{r}
# Make a simple plot of the time-series data
ggplot(data = rap_df)+
  geom_line(mapping = aes(x = Date, y = Value, colour = Variable))+
  labs(title = paste0('Rangeland Analysis Platform (RAP) Vegetation Cover for ', aoi_name |> str_replace('_', ' ')), y = 'Fractional cover (%)') +
  scale_colour_manual(values = c("#BF604A", "#BF8C4A", "#A5C737", "#3757AD", "#0A6E1C"), labels = c("AFG", "BGR", "PFG", "SHR", "TRE"))+
  theme_minimal()+
  theme(legend.position="bottom",
        axis.title.x = element_blank(),
        legend.title=element_blank())
```


## Get 180-day Standardized Precipitation-Evapotranspiration Index (SPEI) data from gridMET Drought.

Here, we will get histograms of 180-day SPEI drought data based on accepted drought categories developed by NOAA NIDIS (drought.gov). This will allow us to make a beautiful stacked bar chart to depict the drought history for our AOI. You can learn more about drought indicators and how the Climate Engine team is helping the BLM to assess drought on our support site: https://support.climateengine.org/article/128-rangeland-drought-assessment

NOTE: I've pre-run the fetching of the histogram data since it can be time-consuming to run live. Skip ahead to the next section to read-in CSVs of the drought data to produce the stacked bar chart.

```{r}
# Generate date list to to extract statistics over
start_year = 2015
end_year = 2023
yearlist = seq(from = start_year, to = end_year, by = 1)
generate_dates <- function(year){
  dates_year <- seq(from = as.Date(paste0(as.character(year), "/1/5")), to = as.Date(paste0(as.character(year), "/12/31")), by = '5 days')
  return(c(as.character(dates_year))) }
dates <- map(yearlist, generate_dates) |> 
  flatten_chr()

# Define Climate Engine API Point 
# Documentation for this endpoint found here: https://docs.climateengine.org/docs/build/html/zonal_statistics.html#rst-zonal-stats-pixel-count-custom-asset
endpoint <- 'zonal_stats/pixel_count/custom_asset'

# Documentation on the gridMET Drought is available here: https://support.climateengine.org/article/45-gridmet-drought
# API Documentation on the Rangeland Analysis Platform: https://docs.climateengine.org/docs/build/html/variables.html#rst-gridmet-drought
dataset <- 'GRIDMET_DROUGHT'
variable <- 'spei180d'

# Generate dataframe of histograms
get_drought_histogram <- function(date){

  print(paste0('Running ', date))

  # Create list of API parameters for request
  params = list(
    dataset = dataset,
    variable = variable,
    end_date = as.character(date),
    area_reducer = 'mean',
    asset_id = asset_id,
    sub_choices = sub_choices,
    filter_by = filter_by,
    bins = '[-2, -1.5, -1.2,-0.7, -0.5, 0.5, 0.7, 1.2, 1.5, 2]')
  
  # Make API request and get data from response
  data <- request(base_url = paste0(root_url, endpoint)) |>
    req_url_query(query = !!!params) |>
    req_headers(Authorization = key) |>
    req_perform() |>
    resp_body_json() |>
    pluck(1)
  
  # Iterate over the SPEI histogram bins to produce dataframe of SPEI histograms
  bins = names(data$spei180d)
  row_df <- tibble('Date' = date)
  for (bin in bins){
    val = data$spei180d[bin][1] |> 
      unlist() |> 
      unname()
    df = tibble(bin = val)
    names(df) <- bin
    row_df = bind_cols(row_df, df)}
  
  return(row_df) }

spei_df <- map(dates, get_drought_histogram) |> 
  bind_rows() %>%
  mutate(across(everything(), replace_na, 0))

# Optionally write out csv
# write_csv(spei_df, paste0('data/', aoi_name, '_', variable, '_', as.character(start_year), '_', as.character(end_year), '.csv'))
```


## Create a stacked barchart of the 180-day SPEI drought categories for the AOI

In the code below we will prepare the dataframe of drought histograms that we produced in the last section for visualization. First, we will pivot the dataframe from wide to long; then, we will calculate the percentage of the area of interest that is in each drought category for each date; last, we will order the categories in the correct order to match the figure referenced at the beginning of this case study.

```{r}
# Optionally read in 180-day SPEI CSV
# If you skipped the above section, you'll need the variables below:
start_year = 2015
end_year = 2023
variable <- 'spei180d'
spei_df <- read_csv(paste0('data/', aoi_name, '_', variable, '_', as.character(start_year), '_', as.character(end_year), '.csv'))

# Pivot spei_df longer for visualizing, select drought categories, and calculate percent of area in each category
spei_df <- spei_df |> 
  select(-'null') |> # remove null column resulting from gridMET drought data issue
  pivot_longer(!Date, names_to = 'Category', values_to = 'Count') |> # pivot from wide to long dataframe
  group_by(Date) |>
  mutate(Pixels = sum(Count)) |> # calculate sum of pixels for each date for generating histogram
  ungroup() |>
  mutate(Category = factor(Category, levels = c('0', '-5', '-4', '-3', '-2', '-1', '1', '2', '3', '4', '5'))) |> # convert Drought Category to a factor for plotting
  mutate(Percent = (Count/Pixels) * 100) |> # calculate percent of pixels in each class for generating histogram
  filter(Category %in% c('0', '-5', '-4', '-3', '-2', '-1')) # filter for drought categories

ggplot(spei_df, mapping = aes(x = Date, y = Percent, fill = Category))+
            geom_bar(stat = 'identity', width = 10)+
            scale_fill_manual(values = c('0' = '#FFFFFF', '-1' = '#FEFE33', '-2' = '#FFD580', '-3' = '#FFA500', '-4' = '#DC143C', '-5' = '#8C000F'), # define drought category colors according to NDMC: https://droughtmonitor.unl.edu/About/AbouttheData/DroughtClassification.aspx
                              labels = c('No drought', 'Abnormally\ndry', 'Moderate', 'Severe', 'Extreme', 'Exceptional'))+ # define drought category names according to NDMC: https://droughtmonitor.unl.edu/About/AbouttheData/DroughtClassification.aspx
            scale_y_continuous(breaks=c(0, 20, 40, 60, 80, 100))+
            labs(title = paste0('180-day SPEI Timeseries Plot for ', aoi_name |> str_replace('_', ' ')), x = '', y = 'Percent', fill = 'Drought\nCategory')+
            theme_classic()+
            theme(legend.key=element_rect(colour="grey"))
```
