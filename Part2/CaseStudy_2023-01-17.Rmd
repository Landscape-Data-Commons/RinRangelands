---
title: 'Case Study: How Does Drought Impact Wind Erosion Risk?'
author: "Sarah McCord, Eric Jensen"
date: "2023-11-30"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview

Now we will use all of the skills we have learned in this workshop in a real world case study. This example walks through using R and publicly available datasets to examine how drought is influencing wind erosion in the Chihuahuan Desert. Our goal is to reproduce Figure 4 from McCord et al. 2024 (***https://doi.org/10.1002/ael2.20120***). First, we will query the Landscape Data Commons for data that fall within the Chihuahuan Desert ecoregion. Then, we will use these data to set a benchmark of expected wind erosion risk in the Chihuahuan Desert. With this benchmark in place, we will apply it to the monitoring data in the Landscape Data Commons and compare those results at multiple spatial scales. Finally, we will incorporate drought and remote-sensing based fractional cover data (***FOR ERIC, Bare ground from RAP could be cool here***), to provide additional context to the field based monitoring data.

![](images/figure_4.jpg)
Library the packages we will use in this demo
```{r}
library(trex) # for fulling data on the Landscape Data Commons
library(sf) # for processing spatial data
library(ggplot2) # for data visualization
library(viridis) # colorblind friendly color palettes
library(httr2) # for Climate Engine API requests
library(mapview) # for visualizing spatial data
```

## 1. Prepare Data
Make sure your working directory is set for Part2 of this workshop, for example: setwd("C:/Users/gharrison/OneDrive - USDA/Documents/R scripts/RinRange_workshop_srm24/Part2")

Now we need to pull in some polygons for our region of interest so that we can set a spatial boundaries on our data.  
*GH will come back and change the fetch edit command so that these data are already downloaded*
*It'd be nice to leave it in, but perhaps commented out?*
```{r}
# get polygons
chihuahuan_desert <- read_sf("MapLayers/ChihuahuanDesert.shp")
sandy_esg <- read_sf("MapLayers/SandyESG.shp")

# extract data from LDC
data <- fetch_ldc_spatial(chihuahuan_desert,
                          data_type = "indicators")

head(data)

# Let's do a bit of clean up and make sure the data fields are formatted properly
data <- data |> dplyr::mutate(DateVisited = lubridate::as_date(DateVisited),
                                    Year = lubridate::year(DateVisited) |> as.factor())


# Make sure the Lat/Long coordinates are numeric
data <- data |> 
  dplyr::mutate(Longitude_NAD83 = as.numeric(Longitude_NAD83),
                Latitude_NAD83 = as.numeric(Latitude_NAD83)) 

```

## 2. Establish benchmark
We want to understand which sites might be susceptible to wind erosion. To do this, we took a subset (~600) of the points in the ecoregion and compared them with wind erosion models and determined that decreases in total foliar cover below 30% are associated with an increase in wind erosion risk. However, we do not want to apply the benchmark to the points used to develop the benchmark. So we will first remove the benchmark generation points. Then we will apply the 30% benchmark to these points. 
``` {r}
# get benchmark developmentdata
benchmark <- read.csv("benchmark_points.csv")

head(benchmark)
```

## 3. Apply benchmark
``` {r}
# remove data that have been used in benchmarking
# check total number of plots
nrow(data)
data <- data |> subset(!PrimaryKey %in% benchmark$PrimaryKey)
nrow(data)

# Apply the benchmark of 30% to the Total Foliar Cover values
data <- data |> dplyr::mutate(evaluated = dplyr::case_when(TotalFoliarCover >30 ~ "Meeting",
                                                           TotalFoliarCover <=30 ~ "Not meeting"
))

# convert to spatial
# move from a st to sf object
# users must specify the columns which are coordinates, and the Coordinate reference system
## TO DO Georgia come back and add links for CRS
data_sf <- sf::st_as_sf(data %>% subset(!is.na(Longitude_NAD83)),
                     coords = c("Longitude_NAD83", "Latitude_NAD83"),
                     crs = st_crs(chihuahuan_desert))
# save these data as a shapefile
sf::st_write(data_sf, "ldc_data.shp", append=F) 


```


For management purposes, we are going to focus on the NM portion of the Chihuahuan Desert and the Sandy Ecological Site Group within that area.
``` {r}
# Subset the data to points found in New Mexico
nm_data <- data_sf |> subset(State == "NM")

# Identify the plots that are found in the Sandy ESG. Load in a table identifying all the SandyESG plots
sandy_plots <- read.csv("SandyESG_plots.csv")

# Join this back to the nm_data
nm_data <- dplyr::left_join(nm_data, 
                            sandy_plots, 
                            by = "PrimaryKey")

#TODO: Georgia we could also do this intersection in R if you want. In ArcGIS we intersected the Sandy Ecological Site group with our data. You can do this  Nelson, could you help me make this intersection work? I think I moved to R because it wasn't going well in R


# Identify the membership of each plot within different polygons (Chihuahuan Desert, MLRA 42, SandyESG and a long-term research site at the Jornada Experimental Range called "NWERN_JER")
nm_data <- nm_data |> dplyr::mutate(Chihuahuan_desert = "yes",
                                    MLRA42 = dplyr::case_when(mlrarsym %in% "42"~ "yes"),
                                    NWERN_JER = dplyr::case_when(ProjectKey %in% "NWERN_JER"~ "yes"))

# To faciliate plotting, it would be helpful if the data were long rather than wide.
nm_data_long <- as.data.frame(nm_data) |>
  tidyr::pivot_longer(cols = c("MLRA42",
                               "SandyESG", 
                               "NWERN_JER", 
                               "Chihuahuan_desert"),
                      names_to = "region",
                      values_to = "region_membership") |>
  subset(region_membership =="yes")|>
  dplyr::mutate(region = factor(region)|>
                  dplyr::recode( "NWERN_JER" = "Site",
                                 "SandyESG" = "ESG", 
                                 "MLRA42" = "MLRA",
                                 "Chihuahuan_desert" = "Ecoregion"))

# We also need region to be a factor for easier plot formatting
nm_data_long$region <- factor(nm_data_long$region,
                              levels = c( "Site", 
                                          "ESG",
                                          "MLRA",
                                          "Ecoregion")
                              )
```

### Plot the results

``` {r}
# box plot
ggplot(nm_data_long |> subset(Year %in% c("2015", "2016", "2017", "2018", "2019", "2020", "2021", "2022") &
                                TotalFoliarCover),
       aes(x = Year,
           y = TotalFoliarCover,
           group = Year)) +
  facet_grid(rows = "region")+
  geom_boxplot()+
  geom_point(aes(color = evaluated))+
  scale_color_manual(values = c("darkblue", "orange"))+
  theme_bw()+
  ylab("Total foliar cover (%)")+
  guides(color=guide_legend(title="Benchmark status"))

# bar chart
ggplot(nm_data_long |> subset(Year %in% c("2015", "2016", "2017", "2018", "2019", "2020", "2021", "2022") &
                                TotalFoliarCover),
       aes(x = Year,
           y = TotalFoliarCover,
           fill = evaluated)) +
  facet_grid(rows = "region")+
  geom_bar(position = "fill", stat = "identity")+
  scale_fill_manual(values = c("darkblue", "orange"))+
  theme_bw()+
  ylab("Proportion of plots")+
  guides(fill=guide_legend(title="Benchmark status"))

# summarize
nm_data_summary <- nm_data_long |> subset(
                         Year %in% c("2015", "2016", "2017", "2018", "2019", "2020", "2021", "2022") &
                         TotalFoliarCover) |>
  dplyr::select(PrimaryKey, region, Year) |>
  dplyr::mutate(Year = as.character(Year)) |>
  dplyr::distinct()|>
  dplyr::group_by(Year, region) |> dplyr::tally()
```

## 4. Climate Data/RAP

### Authenticate to API and run preliminary test

Define Climate Engine API key and run a simple test endpoint to make sure that the key authenticated correctly. The Climate Engine API documentation is published at https://docs.climateengine.org

NOTE: The key below will remain active through February 4, please request a free key to run this in the future: https://docs.climateengine.org/docs/build/html/registration.html

```{r}
# Define root url for Climate Engine API
root_url <- "https://api.climateengine.org/"

# Define key
key <- "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJmcmVzaCI6ZmFsc2UsImlhdCI6MTcwNTg3ODY2NiwianRpIjoiYmEyYTZmMzMtNjdkOS00ODAzLThiOTQtYWQ0Yjg5NmZjZDJkIiwibmJmIjoxNzA1ODc4NjY2LCJ0eXBlIjoiYWNjZXNzIiwic3ViIjoidUZXM0VmMUxhWmI5U0VhUnRPdzVNUzI2UjBTMiIsImV4cCI6MTcwNzE3NDY2Niwicm9sZXMiOiJ1c2VyIiwidXNlcl9pZCI6InVGVzNFZjFMYVpiOVNFYVJ0T3c1TVMyNlIwUzIifQ.WIQZIn97BpFZorL_n6NGvEi_MHg2qMnIUh9Rm6aOcl0"

# Define endpoint
endpoint <- '/home/validate_key'

# Run simple endpoint to get key expiration
test <- request(base_url = paste0(root_url, endpoint)) |>
  req_headers(Authorization = key) |>
  req_perform()

print(resp_raw(test))

# Clean up unneeded objects
rm(test, endpoint)
```


### Import the wind erosion area of interest

This section of the notebook runs using two sample areas of interest for the Sandy Ecological Site Group and for the Chihuahuan Desert ecoregion, but you can run similar analysis for any shapefile. We will import the shapefile as an sf object and upload it to Google Earth Engine and then visualize thee seleted area of interest (aoi) on a leaflet map using the mapview package. You can also modify which area of interest you want to analyze in this code chunk.

```{r}
# Set AOI name as either 'Sandy_ESG' or 'Chihuahuan_Desert'
aoi_name = 'Sandy_ESG'

# Import the Sandy ESG shapefile and select an individual area of interest. 
aoi <- st_read(paste0('MapLayers/', aoi_name |> str_remove('_'), '.shp')) |>
  st_simplify(preserveTopology = TRUE) |> 
  st_transform(crs = "OGC:CRS84") 

# Define variables based on aoi_name
if(aoi_name == 'Sandy_ESG'){
  asset_id = 'users/EricRJensen-DRI/RinRangelands/SandyESG'
  sub_choices = '["SandyESG"]'
  filter_by = 'id'
} else if(aoi_name == 'Chihuahuan_Desert'){
  asset_id = 'users/EricRJensen-DRI/RinRangelands/ChihuahuanDesert'
  sub_choices = '["Chihuahuan Semi-Desert Province"]'
  filter_by = 'PROVINCE'
}

# Visualize the allotments on a leaflet map using mapview package
mapview(aoi)
```


### Make API requests to get Rangeland Analysis Platform (RAP) fractional vegetation cover data for annual forbs and grasses (AFG), perennial forbs and grasses (PFG), shrubs (SHR), trees (TRE), and bare ground (BGR)

#### Fetch RAP data

Here we will iterate over a list of vegetation types to get an annual timeseries of vegetation cover for our area of interest. There are a few steps included here to clean up the data returned from the API. Requests are sent to the API in the JSON format and the resulting data is returned in the JSON format, so several lines of code convert the JSON data into an R dataframe.

Resources:
- API Documentation for the endpoint used in this section is found here: https://docs.climateengine.org/docs/build/html/timeseries.html#rst-timeseries-native-custom-asset
- API Documentation for the Rangeland Analysis Platform dataset is found here: https://docs.climateengine.org/docs/build/html/variables.html#rst-rap-vegetation-cover
- Documentation on the Rangeland Analysis Platform is available through https://rangelands.app

```{r}
# Define Climate Engine API Point 
endpoint <- 'timeseries/native/custom_asset'

# Define dataset for function below

dataset <- "RAP_COVER"

# Make a list of RAP variable options â€” we will loop over these using the function below to return all of the data
variables <- c('AFG', 'BGR', 'PFG', 'SHR', 'TRE')
 
# Function to return data frame of RAP time-series
get_rap_df <- function(variable){
  
  print(paste0("Requesting data for ", variable))
  
  # Create list of API parameters for request
  params <- list(
    dataset = dataset,
    variable = variable,
    start_date = '1986-01-01',
    end_date = '2022-12-31',
    area_reducer = 'mean',
    asset_id = asset_id,
    sub_choices = sub_choices,
    filter_by = filter_by
    )
  
  # Make API request and get data from response
  data <- request(base_url = paste0(root_url, endpoint)) |> # define URL to make request to
    req_url_query(query = !!!params) |> # define paramaters for request
    req_headers(Authorization = key) |> # set authentication key as a header for request
    req_perform() |> # perform request
    resp_body_json() |> # parse result into JSON
    pluck(1, 'Data') # Get RAP data from the JSON
  
  # Convert JSON to dataframe
  # For each date in data response, convert to one row dataframe and then bind them together
  generate_data_frame <- function(item){
    
    return(tibble(Date = as.Date(item$Date),
                  Variable = as.factor(names(item[2]) |> str_sub(1,4)),
                  Value = item[[2]])) }

  df <- map(data, generate_data_frame) |> 
    bind_rows()
  
  return(df) }

# Map API function over list of RAP variables to get timeseries
rap_df <- map(variables, get_rap_df) |>
  bind_rows() # Combine list of dataframes into an individual dataframe
print(rap_df)
```


#### Visualize the RAP data as timeseries similar for each vegetation type

Make simple plots of the RAP time-series data for the Chihuahuan Desert or Sandy ESG, depending on the user selection. The plot produces is similar to the one produced in the RAP web application.

Note: We could also visualize the data as a stacked bar chart, similar to with the drought data below, but decided to visualize them as lines do do something different.

```{r}
# Make a simple plot of the time-series data
ggplot(data = rap_df)+
  geom_line(mapping = aes(x = Date, y = Value, colour = Variable))+
  labs(title = paste0('Rangeland Analysis Platform (RAP) Vegetation Cover for ', aoi_name |> str_replace('_', ' ')), y = 'Fractional cover (%)') +
  scale_colour_manual(values = c("#BF604A", "#BF8C4A", "#A5C737", "#3757AD", "#0A6E1C"), labels = c("AFG", "BGR", "PFG", "SHR", "TRE"))+
  theme_minimal()+
  theme(legend.position="bottom",
        axis.title.x = element_blank(),
        legend.title=element_blank())
```


### Make API request to get Standardized Precipitation-Evapotranspiration Index (SPEI) data from gridMET Drought.

#### Fetch 180-day SPEI data from gridMET Drought.

Here, we will get histograms of 180-day SPEI drought data based on accepted drought categories developed by NOAA NIDIS (drought.gov). This will allow us to make a beautiful stacked bar chart to depict the drought history for our AOI. You can learn more about drought indicators and how the Climate Engine team is helping the BLM to assess drought on our support site: https://support.climateengine.org/article/128-rangeland-drought-assessment

NOTE: I've pre-run the fetching of the histogram data since it can be time-consuming to run live. Skip ahead to the next section to read-in CSVs of the drought data to produce the stacked bar chart.

```{r}
# Generate date list to to extract statistics over
start_year = 2015
end_year = 2023
yearlist = seq(from = start_year, to = end_year, by = 1)
generate_dates <- function(year){
  dates_year <- seq(from = as.Date(paste0(as.character(year), "/1/5")), to = as.Date(paste0(as.character(year), "/12/31")), by = '5 days')
  return(c(as.character(dates_year))) }
dates <- map(yearlist, generate_dates) |> 
  flatten_chr()

# Define Climate Engine API Point 
# Documentation for this endpoint found here: https://docs.climateengine.org/docs/build/html/zonal_statistics.html#rst-zonal-stats-pixel-count-custom-asset
endpoint <- 'zonal_stats/pixel_count/custom_asset'

# Documentation on the gridMET Drought is available here: https://support.climateengine.org/article/45-gridmet-drought
# API Documentation on the Rangeland Analysis Platform: https://docs.climateengine.org/docs/build/html/variables.html#rst-gridmet-drought
dataset <- 'GRIDMET_DROUGHT'
variable <- 'spei180d'

# Generate dataframe of histograms
get_drought_histogram <- function(date){

  print(paste0('Running ', date))

  # Create list of API parameters for request
  params = list(
    dataset = dataset,
    variable = variable,
    end_date = as.character(date),
    area_reducer = 'mean',
    asset_id = asset_id,
    sub_choices = sub_choices,
    filter_by = filter_by,
    bins = '[-2, -1.5, -1.2,-0.7, -0.5, 0.5, 0.7, 1.2, 1.5, 2]')
  
  # Make API request and get data from response
  data <- request(base_url = paste0(root_url, endpoint)) |>
    req_url_query(query = !!!params) |>
    req_headers(Authorization = key) |>
    req_perform() |>
    resp_body_json() |>
    pluck(1)
  
  # Iterate over the SPEI histogram bins to produce dataframe of SPEI histograms
  bins = names(data$spei180d)
  row_df <- tibble('Date' = date)
  for (bin in bins){
    val = data$spei180d[bin][1] |> 
      unlist() |> 
      unname()
    df = tibble(bin = val)
    names(df) <- bin
    row_df = bind_cols(row_df, df)}
  
  return(row_df) }

spei_df <- map(dates, get_drought_histogram) |> 
  bind_rows() %>%
  mutate(across(everything(), replace_na, 0))

# Optionally write out csv
# write_csv(spei_df, paste0('data/', aoi_name, '_', variable, '_', as.character(start_year), '_', as.character(end_year), '.csv'))
```


#### Create a stacked barchart of the 180-day SPEI drought categories for the AOI

In the code below we will prepare the dataframe of drought histograms that we produced in the last section for visualization. First, we will pivot the dataframe from wide to long; then, we will calculate the percentage of the area of interest that is in each drought category for each date; last, we will order the categories in the correct order to match the figure referenced at the beginning of this case study.

```{r}
# Optionally read in 180-day SPEI CSV
# If you skipped the above section, you'll need the variables below:
start_year = 2015
end_year = 2023
variable <- 'spei180d'
spei_df <- read_csv(paste0('data/', aoi_name, '_', variable, '_', as.character(start_year), '_', as.character(end_year), '.csv'))

# Pivot spei_df longer for visualizing, select drought categories, and calculate percent of area in each category
spei_df <- spei_df |> 
  select(-'null') |> # remove null column resulting from gridMET drought data issue
  pivot_longer(!Date, names_to = 'Category', values_to = 'Count') |> # pivot from wide to long dataframe
  group_by(Date) |>
  mutate(Pixels = sum(Count)) |> # calculate sum of pixels for each date for generating histogram
  ungroup() |>
  mutate(Category = factor(Category, levels = c('0', '-5', '-4', '-3', '-2', '-1', '1', '2', '3', '4', '5'))) |> # convert Drought Category to a factor for plotting
  mutate(Percent = (Count/Pixels) * 100) |> # calculate percent of pixels in each class for generating histogram
  filter(Category %in% c('0', '-5', '-4', '-3', '-2', '-1')) # filter for drought categories

ggplot(spei_df, mapping = aes(x = Date, y = Percent, fill = Category))+
            geom_bar(stat = 'identity', width = 10)+
            scale_fill_manual(values = c('0' = '#FFFFFF', '-1' = '#FEFE33', '-2' = '#FFD580', '-3' = '#FFA500', '-4' = '#DC143C', '-5' = '#8C000F'), # define drought category colors according to NDMC: https://droughtmonitor.unl.edu/About/AbouttheData/DroughtClassification.aspx
                              labels = c('No drought', 'Abnormally\ndry', 'Moderate', 'Severe', 'Extreme', 'Exceptional'))+ # define drought category names according to NDMC: https://droughtmonitor.unl.edu/About/AbouttheData/DroughtClassification.aspx
            scale_y_continuous(breaks=c(0, 20, 40, 60, 80, 100))+
            labs(title = paste0('180-day SPEI Timeseries Plot for ', aoi_name |> str_replace('_', ' ')), x = '', y = 'Percent', fill = 'Drought\nCategory')+
            theme_classic()+
            theme(legend.key=element_rect(colour="grey"))
```

